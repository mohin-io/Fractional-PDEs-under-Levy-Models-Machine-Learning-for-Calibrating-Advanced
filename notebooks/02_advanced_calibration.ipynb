{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Calibration: Bayesian MCMC with Uncertainty Quantification\n",
    "\n",
    "This notebook demonstrates **Bayesian calibration** using Markov Chain Monte Carlo (MCMC) methods. Unlike the neural network approach (which provides point estimates), Bayesian methods give us **full posterior distributions** and **uncertainty quantification**.\n",
    "\n",
    "## Why Bayesian Calibration?\n",
    "\n",
    "- **Uncertainty quantification**: Get credible intervals, not just point estimates\n",
    "- **Incorporate prior knowledge**: Use domain expertise as priors\n",
    "- **Model comparison**: Compute Bayes factors to choose between models\n",
    "- **Robust to noise**: Naturally handles noisy market data\n",
    "\n",
    "## Trade-offs\n",
    "\n",
    "- ‚è±Ô∏è **Slower**: ~1-5 minutes vs ~10ms for neural network\n",
    "- üìä **More informative**: Full distributions vs point estimates\n",
    "- üéØ **Best for**: Critical calibrations, research, model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and TensorFlow Probability\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Import calibration modules\n",
    "from models.pricing_engine.levy_models import variance_gamma_char_func\n",
    "from models.pricing_engine.fourier_pricer import price_surface\n",
    "from models.bayesian_calibration.mcmc import BayesianCalibrator\n",
    "from models.bayesian_calibration.diagnostics import compute_rhat, compute_ess\n",
    "from models.bayesian_calibration.uncertainty_propagation import propagate_uncertainty_single_option\n",
    "\n",
    "# Plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Probability version: {tfp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Market Data\n",
    "\n",
    "We'll create a \"ground truth\" option surface with known parameters, then add market noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters (our ground truth)\n",
    "sigma_true = 0.25\n",
    "nu_true = 0.35\n",
    "theta_true = -0.15\n",
    "\n",
    "# Market parameters\n",
    "S0 = 100.0\n",
    "r = 0.05\n",
    "\n",
    "# Define grid (smaller for faster MCMC)\n",
    "strikes = np.array([90, 95, 100, 105, 110])\n",
    "maturities = np.array([0.25, 0.5, 1.0])\n",
    "\n",
    "# Generate true surface\n",
    "char_func_true = variance_gamma_char_func(sigma_true, nu_true, theta_true)\n",
    "true_surface = price_surface(S0, strikes, maturities, r, char_func_true)\n",
    "\n",
    "# Add realistic market noise (bid-ask spread ~0.5%)\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 0.005 * true_surface)\n",
    "observed_surface = true_surface + noise\n",
    "\n",
    "print(\"Ground Truth Parameters:\")\n",
    "print(f\"  œÉ (sigma) = {sigma_true}\")\n",
    "print(f\"  ŒΩ (nu)    = {nu_true}\")\n",
    "print(f\"  Œ∏ (theta) = {theta_true}\")\n",
    "print(f\"\\nOption surface shape: {observed_surface.shape}\")\n",
    "print(f\"Price range: ${observed_surface.min():.2f} - ${observed_surface.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Observed vs True Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for i, T in enumerate(maturities):\n",
    "    ax = axes[i]\n",
    "    ax.plot(strikes, true_surface[:, i], 'o-', label='True', linewidth=2, markersize=8)\n",
    "    ax.plot(strikes, observed_surface[:, i], 's--', label='Observed (noisy)', \n",
    "            linewidth=2, markersize=6, alpha=0.7)\n",
    "    ax.set_xlabel('Strike', fontsize=11)\n",
    "    ax.set_ylabel('Option Price', fontsize=11)\n",
    "    ax.set_title(f'T = {T} years', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Prior Distributions\n",
    "\n",
    "We encode our prior beliefs about the parameters:\n",
    "\n",
    "- **œÉ (sigma)**: Volatility is typically 0.1-0.5 ‚Üí LogNormal prior\n",
    "- **ŒΩ (nu)**: Variance rate is positive, mean ~0.3 ‚Üí Gamma prior  \n",
    "- **Œ∏ (theta)**: Drift is typically negative for equities ‚Üí Normal prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bayesian calibrator\n",
    "calibrator = BayesianCalibrator(model_name='VarianceGamma')\n",
    "\n",
    "# Build prior\n",
    "prior = calibrator.build_prior()\n",
    "\n",
    "# Sample from prior to visualize\n",
    "prior_samples = prior.sample(5000)\n",
    "\n",
    "# Plot prior distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "param_names = ['sigma', 'nu', 'theta']\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(param_names, axes)):\n",
    "    samples = prior_samples[name].numpy()\n",
    "    ax.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline([sigma_true, nu_true, theta_true][i], color='red', \n",
    "                linestyle='--', linewidth=2, label='True value')\n",
    "    ax.set_xlabel(f'{name}', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'Prior: {name}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Prior distributions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run MCMC Sampling (NUTS)\n",
    "\n",
    "We'll use the **No-U-Turn Sampler (NUTS)**, a state-of-the-art Hamiltonian Monte Carlo algorithm.\n",
    "\n",
    "‚ö†Ô∏è **Note**: This cell may take 2-5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Running MCMC sampling...\")\n",
    "print(\"This may take 2-5 minutes. Please wait...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run MCMC\n",
    "posterior_samples, diagnostics = calibrator.run_mcmc(\n",
    "    observed_prices=observed_surface.flatten(),\n",
    "    strikes=strikes,\n",
    "    maturities=maturities,\n",
    "    S0=S0,\n",
    "    r=r,\n",
    "    num_samples=2000,      # Number of samples per chain\n",
    "    num_burnin=1000,       # Burn-in samples to discard\n",
    "    num_chains=4,          # Number of parallel chains\n",
    "    step_size=0.01         # Initial step size\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "sampling_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úì MCMC sampling complete!\")\n",
    "print(f\"  Total time: {sampling_time:.1f} seconds\")\n",
    "print(f\"  Samples per chain: {posterior_samples['sigma'].shape[0]}\")\n",
    "print(f\"  Number of chains: {posterior_samples['sigma'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convergence Diagnostics\n",
    "\n",
    "Before trusting the results, we must verify MCMC convergence:\n",
    "\n",
    "- **R-hat**: Should be < 1.01 (measures convergence across chains)\n",
    "- **ESS**: Effective Sample Size (should be > 400 for reliable estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Parameter':<10} {'R-hat':<12} {'ESS':<12} {'Status'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for param in ['sigma', 'nu', 'theta']:\n",
    "    chains = posterior_samples[param]\n",
    "    rhat = compute_rhat(chains)\n",
    "    ess = compute_ess(chains)\n",
    "    \n",
    "    status = \"‚úì Good\" if (rhat < 1.01 and ess > 400) else \"‚ö† Check\"\n",
    "    print(f\"{param:<10} {rhat:<12.4f} {ess:<12.0f} {status}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  R-hat < 1.01: Chains have converged ‚úì\")\n",
    "print(\"  ESS > 400: Sufficient effective samples for inference ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trace Plots\n",
    "\n",
    "Visual inspection of chain mixing and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "param_names = ['sigma', 'nu', 'theta']\n",
    "true_values = [sigma_true, nu_true, theta_true]\n",
    "\n",
    "for i, (param, true_val) in enumerate(zip(param_names, true_values)):\n",
    "    chains = posterior_samples[param]\n",
    "    \n",
    "    # Left: Trace plot\n",
    "    ax_trace = axes[i, 0]\n",
    "    for chain_idx in range(chains.shape[1]):\n",
    "        ax_trace.plot(chains[:, chain_idx], alpha=0.6, linewidth=0.8)\n",
    "    ax_trace.axhline(true_val, color='red', linestyle='--', linewidth=2, label='True')\n",
    "    ax_trace.set_ylabel(param, fontsize=11)\n",
    "    ax_trace.set_xlabel('Iteration', fontsize=10)\n",
    "    ax_trace.set_title(f'Trace: {param}', fontsize=11, fontweight='bold')\n",
    "    ax_trace.legend()\n",
    "    ax_trace.grid(alpha=0.3)\n",
    "    \n",
    "    # Right: Autocorrelation\n",
    "    ax_acf = axes[i, 1]\n",
    "    samples_flat = chains.flatten()\n",
    "    from pandas.plotting import autocorrelation_plot\n",
    "    autocorrelation_plot(pd.Series(samples_flat), ax=ax_acf, color='steelblue')\n",
    "    ax_acf.set_title(f'Autocorrelation: {param}', fontsize=11, fontweight='bold')\n",
    "    ax_acf.set_xlim(0, 100)\n",
    "    ax_acf.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Good mixing: chains overlap and explore parameter space\")\n",
    "print(\"Low autocorrelation: samples are nearly independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Posterior Distributions\n",
    "\n",
    "The main result: **posterior distributions** for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (param, true_val, ax) in enumerate(zip(param_names, true_values, axes)):\n",
    "    samples_flat = posterior_samples[param].flatten()\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(samples_flat, bins=50, density=True, alpha=0.7, \n",
    "            edgecolor='black', color='steelblue')\n",
    "    \n",
    "    # True value\n",
    "    ax.axvline(true_val, color='red', linestyle='--', linewidth=2.5, label='True value')\n",
    "    \n",
    "    # Posterior mean\n",
    "    posterior_mean = np.mean(samples_flat)\n",
    "    ax.axvline(posterior_mean, color='green', linestyle='-', linewidth=2.5, label='Posterior mean')\n",
    "    \n",
    "    # 95% HDI (Highest Density Interval)\n",
    "    hdi_lower = np.percentile(samples_flat, 2.5)\n",
    "    hdi_upper = np.percentile(samples_flat, 97.5)\n",
    "    ax.axvspan(hdi_lower, hdi_upper, alpha=0.2, color='green', label='95% HDI')\n",
    "    \n",
    "    ax.set_xlabel(param, fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'Posterior: {param}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Parameter':<10} {'True':<10} {'Mean':<10} {'Std':<10} {'95% HDI'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for param, true_val in zip(param_names, true_values):\n",
    "    samples_flat = posterior_samples[param].flatten()\n",
    "    mean = np.mean(samples_flat)\n",
    "    std = np.std(samples_flat)\n",
    "    hdi_lower = np.percentile(samples_flat, 2.5)\n",
    "    hdi_upper = np.percentile(samples_flat, 97.5)\n",
    "    \n",
    "    print(f\"{param:<10} {true_val:<10.4f} {mean:<10.4f} {std:<10.4f} [{hdi_lower:.4f}, {hdi_upper:.4f}]\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Uncertainty Propagation to Option Pricing\n",
    "\n",
    "We can propagate parameter uncertainty to get **prediction intervals** for option prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific option to analyze\n",
    "K_test = 100.0\n",
    "T_test = 1.0\n",
    "\n",
    "# Propagate uncertainty\n",
    "result = propagate_uncertainty_single_option(\n",
    "    posterior_samples=posterior_samples,\n",
    "    model_name='VarianceGamma',\n",
    "    S0=S0,\n",
    "    K=K_test,\n",
    "    T=T_test,\n",
    "    r=r,\n",
    "    num_samples=1000\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"OPTION PRICING WITH UNCERTAINTY (K={K_test}, T={T_test})\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPredicted price: ${result['mean']:.4f}\")\n",
    "print(f\"Standard deviation: ${result['std']:.4f}\")\n",
    "print(f\"95% Credible interval: [${result['hdi_95_lower']:.4f}, ${result['hdi_95_upper']:.4f}]\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot price distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(result['price_samples'], bins=50, density=True, alpha=0.7, \n",
    "        edgecolor='black', color='steelblue')\n",
    "ax.axvline(result['mean'], color='green', linestyle='-', linewidth=2.5, label='Mean')\n",
    "ax.axvspan(result['hdi_95_lower'], result['hdi_95_upper'], \n",
    "           alpha=0.2, color='green', label='95% HDI')\n",
    "ax.set_xlabel('Option Price', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title(f'Predictive Distribution for Option (K={K_test}, T={T_test})', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Parameter Correlations (Corner Plot)\n",
    "\n",
    "Visualize joint posterior distributions and parameter correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for corner plot\n",
    "samples_array = np.column_stack([\n",
    "    posterior_samples['sigma'].flatten(),\n",
    "    posterior_samples['nu'].flatten(),\n",
    "    posterior_samples['theta'].flatten()\n",
    "])\n",
    "\n",
    "# Create corner plot using seaborn pairplot\n",
    "df_samples = pd.DataFrame(samples_array, columns=['sigma', 'nu', 'theta'])\n",
    "g = sns.pairplot(df_samples, diag_kind='hist', plot_kws={'alpha': 0.3, 's': 5}, \n",
    "                 diag_kws={'bins': 40, 'edgecolor': 'black'})\n",
    "g.fig.suptitle('Posterior Parameter Correlations', y=1.02, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add true values\n",
    "for i, param in enumerate(['sigma', 'nu', 'theta']):\n",
    "    for j in range(3):\n",
    "        if i != j:\n",
    "            g.axes[i, j].axhline(true_values[i], color='red', linestyle='--', alpha=0.7)\n",
    "            g.axes[i, j].axvline(true_values[j], color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_samples.corr()\n",
    "print(\"\\nPosterior Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. ‚úÖ Generated synthetic market data with realistic noise\n",
    "2. ‚úÖ Defined informative prior distributions for VG parameters\n",
    "3. ‚úÖ Ran MCMC sampling using the NUTS algorithm\n",
    "4. ‚úÖ Verified convergence using R-hat and ESS diagnostics\n",
    "5. ‚úÖ Visualized posterior distributions and parameter correlations\n",
    "6. ‚úÖ Propagated uncertainty to option pricing predictions\n",
    "7. ‚úÖ Obtained **credible intervals** for all parameters and predictions\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "- **Bayesian methods provide full uncertainty quantification**\n",
    "- **MCMC is slower (~minutes) but more informative than neural networks (~ms)**\n",
    "- **Best use case**: Critical calibrations where uncertainty matters (e.g., risk management, regulatory reporting)\n",
    "- **Hybrid approach**: Use neural networks for speed, Bayesian for validation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try CGMY model calibration\n",
    "- Experiment with different priors\n",
    "- Compare Bayesian vs ML calibration on real market data\n",
    "- Implement model comparison using WAIC or Bayes factors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
